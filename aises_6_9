<style type="text/css">
    table.tableLayout{
        margin: auto;
        border: 1px solid;
        border-collapse: collapse;
        border-spacing: 1px;
        caption-side: bottom;
    }

    table.tableLayout tr{
        border: 1px solid;
        border-collapse: collapse;
        padding: 5px;
    }

    table.tableLayout th{
        border: 1px solid;
        border-collapse: collapse;
        padding: 3px;
    }

    table.tableLayout td{
        border: 1px solid;
        padding: 5px;
    }
</style>

<h1 id="sec:uncertainty">6.9 Moral Uncertainty</h1>
<h2 id="making-decisions-under-moral-uncertainty">6.9.1 Making Decisions Under
Moral Uncertainty</h2>
<p>This section considers how to make decisions under moral
uncertainty–—how to act morally when we are unsure which moral view is
correct. Although ignoring our uncertainty may be a comfortable approach
in daily life, there are situations where it is crucial to identify the
best decision. We will start by considering our uncertainties about
morality and the idea of reasonable pluralism, which acknowledges the
potential co-existence of multiple reasonable moral theories such as
ethical theories, common-sense morality, and religious teachings.<p>
The main aim of the section is to recognize that there are different
reasonable moral beliefs and think about what to do with them. We will
explore uncertainties about moral truths, why they matter in moral
decision-making for both humans and AI, and how to deal with them. We
will look at a few proposed solutions, including <em>My Favorite
Theory</em>, <em>Maximize Expected Choice-Worthiness (MEC)</em>, and
<em>Moral Parliament</em> <span class="citation"
data-cites="macaskill2020moral">[1]</span>. These approaches will be
compared and evaluated in terms of their ability to help us make moral
decisions under uncertainty. We will then briefly explore how we might use one of these solutions, 
the idea of a Moral Parliament, to enable AI systems to capture moral uncertainty in their decision-making.</p>
<h3 id="dealing-with-moral-uncertainty">Dealing with Moral
Uncertainty</h3>
<p><strong>Moral theories</strong> Moral theories are systematic attempts to provide a general account of moral principles that apply universally. Good moral theories should provide a coherent, consistent framework for determining whether an action is right or wrong. A basic background understanding of some of the most commonly held moral theories provides a useful foundation for thinking about the kinds of goals or ideals that we wish AI systems to promote. Without this background, there is a risk that developers and users of AI systems may  jump to conclusions about these topics with a false sense of certainty and without considering many potential considerations that could change their decisions. It would be highly inefficient for those developing AI systems or trying to make them safer to attempt to re-invent moral systems, without learning from the large existing body of philosophical work on these topics. We do not have space here to explore moral theories in depth, but we provide suggestions in the Recommended Reading section for those looking for a more detailed introduction to these topics.</p>

<p>There are many different types of moral theories, each of which emphasizes different moral values and considerations. Consequentialist theories like utilitarianism hold that the morality of an action is determined by its consequences or outcomes. Utilitarianism places an emphasis on maximizing everyone’s wellbeing. Utilitarianism claims that consequences (and only consequences) determine whether an action is right or wrong, that wellbeing is the only intrinsic good, that everyone’s wellbeing should be weighed impartially, and that we should maximize wellbeing.</p>

<p>By contrast, under deontological theories, some actions (like lying or killing) are simply wrong, and they cannot be justified by the good consequences that they might bring about. Deontology is the name for a family of ethical theories that deny that the rightness of actions is solely determined by their consequences. Deontological theories are systems of rules or obligations that constrain moral behavior. The term deontology encompasses religious ethical theories, non-religious ethical theories, and principles and rules that are not part of theories at all. These theories give obligations and constraints priority over consequences.</p>

<p>Other moral theories may emphasize other values. For example, social contract theory (or contractarianism) focuses on contracts---or, more generally, hypothetical agreements between members of a society--—as the foundation of ethics. A rule such as ``do not kill'' is morally right, according to a social contract theorist, because individuals would agree that the adoption of this rule is in their mutual best interest, and would therefore insert it into a social contract underpinning that society.</p>

<p><strong>Moral uncertainty requires us to consider multiple moral
theories.</strong> Individuals may have varying degrees of belief in
different moral theories, known as <em>credence</em>.</p>
<p><strong>Credence is the probability assigned by an individual of the
chance a theory is true.</strong> Someone may have a high degree of
credence (70%) in utilitarianism, meaning they believe that maximizing
utility is likely to be the most important moral principle. However,
they may also have some credence in deontological rules, believing they
are plausible but less likely to be true than utilitarianism (30%).
Since ethical theories are not flawless, often arriving at intuitively
questionable conclusions, it is valuable to consider multiple
perspectives when making moral decisions.</p>
<p><strong>Living a good life can require a combination of insights from
multiple moral theories.</strong> We often find ourselves balancing
different kinds of deeds: creating pleasure for people, respecting
autonomy, and adhering to societal moral norms. Abiding by a
<em>reasonable pluralism</em> means accepting that moral guidance from
different sources may conflict while still offering value.</p>
<p><strong>In high-stakes moral decisions, such as in healthcare or AI
design, reasonable pluralism alone may fall short.</strong> When a
healthcare administrator faces the tough choice of allocating limited
resources between hospitals, we might want them to seriously consider
the ethics of what they are doing rather than just go with conflicting
wisdom that seems reasonable at first glance. Therefore, we must think
hard about how to make decisions under moral uncertainty–—seeking truth
for crucial decisions is vital.</p>
<p><strong>If AI is unable to account moral uncertainty, harmful
outcomes are likely.</strong> As AI systems become increasingly
advanced, they are likely to become better at optimising for the goals
that we set them, finding more creative and powerful solutions to
achieve these. However, if they pursue very narrowly specified goals,
there is a risk that these solutions come at the expense of other
important values that we failed to adequately include as part of their
goals. 
<p><strong>Moral disagreement is a reason for making AI systems cautious.</strong> Given that philosophers have not yet converged on a single
consistent moral theory that can take account of all relevant arguments
and intuitions, it seems important for us to design AI systems without
encoding a false sense of certainty about moral issues that could lead
to unintended consequences. To counter such problems, we need AI systems
to recognize that a broad range of moral perspectives might be valid—in
other words, we need AI systems to acknowledge moral uncertainty. This
presents another challenge: how should we rationally balance the
recommendations of different ethical theories? This is the question of
moral uncertainty.</p>
<h2 id="how-should-we-approach-moral-uncertainty">6.9.1 How Should We Approach
Moral Uncertainty?</h2>
<p><strong>There are several potential solutions to moral
uncertainty.</strong> Faced with ethical uncertainty, we can turn to
systematic approaches, using our estimates of how theories judge
different actions and how likely these theories are. This section
explores three potential solutions to moral uncertainty. The first is
adopting a favored moral theory that aligns with personal beliefs
(<em>My Favorite Theory</em>). The second is aiming for the highest
average moral value by calculating the expected choice-worthiness of
each option (<em>Maximize Expected Choice-Worthiness</em>). The third is
treating the decision as a negotiation in a parliament, considering
multiple moral views to find a mutually acceptable solution (<em>Moral
Parliament</em>).<p>
Consider whether we should we lie to save a life. Imagine that a
notorious murderer asks Alex where his friend, Jordan, is. Alex knows
that revealing Jordan’s location will likely lead to his friend’s death,
while lying would save Jordan’s life. However, lying is morally
questionable. Alex must decide which action to take. He is unsure, and
considers the recommendations of the three moral theories he has some
credence in: utilitarianism, deontology, and contractarianism. Alex
thinks utilitarianism, which values lying to save a life highly, is the
most likely to be true: he has 60% credence in it. Deontology, which
Alex has 30% credence in, strongly disapproves of lying, even to save a
life, and contractarianism, which Alex has 10% credence in, moderately
approves of lying in this situation. This information is represented in
Table 7.2.</p>
<p>
<br>
<div id="tab:lying">
<table class="tableLayout">
<caption style="text-align: center;">Table 7.2: Example: Lying to save a life.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">Utilitarianism</th>
<th style="text-align: center;">Deontology</th>
<th style="text-align: center;">Contractarianism</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">What is Alex’s estimate of the chance this
theory is true</td>
<td style="text-align: center;">60%</td>
<td style="text-align: center;">30%</td>
<td style="text-align: center;">10%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Does this theory like lying to save a
life?</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
</tbody>
</table>
</div>
</br>
</p>
<p><strong>Under My Favorite Theory (MFT), Alex would pick whatever
utilitarianism recommends.</strong> Alex thinks that utilitarianism is
the most likely to be true. The MFT approach is to follow the
prescription of the theory we believe is the closest to a moral truth.
Intuitively, many people do this already when thinking about morality.
The advantage of MFT is its simplicity: it is relatively simple and
straightforward to implement. It does not require complex calculations
or a detailed understanding of different moral perspectives. This
approach can be useful when the level of moral uncertainty is low, and
it is clear which theory or option is the best choice.</p>
<p><strong>However, following MFT can lead to harmful single-mindedness
or overconfidence.</strong> It can be difficult to put aside personal
biases or to recognize when one’s own moral beliefs are fallible
(individuals tend to defend and rationalize their chosen theories). The
key issue with MFT is that it can discard relevant information, such as
when the credences in two theories are close, but their judgments of an
action vastly differ <span class="citation"
data-cites="lloyd2022property">[2]</span>. Imagine having 51% credence
in contractarianism, which mildly supports lying to save a life, and 49%
credence in Deontology, which views it as profoundly immoral. MFT
suggests following the marginally favored theory, even though the
potential harm, according to the second theory, is much larger. This
seems counterintuitive, indicating that MFT might not always provide the
most sensible approach for navigating moral uncertainty.</p>
<p><strong>Maximize Expected Choice-Worthiness (MEC) gives us a
procedure to follow.</strong> MEC tells us that to determine how to act,
we need to do the following two things:</p>
<ol>
<li><p><strong>Determine choice-worthiness.</strong> Choice-worthiness
is a measure of the overall desirability or value of an option—in this
context, it is how morally good a choice is. This is an expression of
the size of the moral value of an action. We can represent the
choice-worthiness of an action as a number. For instance, we might think
that under utilitarianism, the choice-worthiness of murder could be
-1000, of littering could be -2, of helping an old lady cross the street
could be +10, and of averting existential risk could be +10000.</p></li>
<li><p><strong>Multiply choice-worthiness by credence.</strong> We can
consider the average choice-worthiness of an action, weighted by how
likely we think each theory is to be true. This gives us a sense of our
best guess of the average moral value of an action, much like how we
considered expected utility when discussing utilitarianism. Table 11 has
each theory’s choice-worthiness value for lying to save a life. As
before, utilitarianism highly values lying to save a life (+500),
deontology strongly disapproves of it (-1000), and contractualism
moderately approves of it (+100). In the row beneath these values are
the credence probability-weighted judgments for Alex. The total
calculation is</p></li>
</ol>
<p><span class="math display">$$\frac{60}{100} \cdot 500 +
\frac{30}{100} (-1000) + \frac{10}{100} \cdot 10 = 300 - 300 + 10 =
10$$</span></p>
<p>Under MEC, Alex would choose to lie, because given Alex’s credence in
each moral theory and his determination of how each moral theory judges
lying to save a life, lying has a higher expected choice-worthiness.
Alex would lie because he judges that, on average, lying is the best
possible action.</p>
<p>
<div id="tab:lying-2">
<table class="tableLayout">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">Utilitarianism</th>
<th style="text-align: center;">Deontology</th>
<th style="text-align: center;">Contractarianism</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">What is Alex’s estimate of the chance this
theory is true</td>
<td style="text-align: center;">60%</td>
<td style="text-align: center;">30%</td>
<td style="text-align: center;">10%</td>
</tr>
<tr class="even">
<td style="text-align: left;">How much does this theory like lying to
save a life</td>
<td style="text-align: center;">+500</td>
<td style="text-align: center;">-1000</td>
<td style="text-align: center;">+100</td>
</tr>
<tr class="odd">
<td style="text-align: left;">What is the probability-weighted
judgement?</td>
<td style="text-align: center;">+300</td>
<td style="text-align: center;">-300</td>
<td style="text-align: center;">+ 10</td>
</tr>
</tbody>
<caption>Table 7.3: Example: Lying to save a life.</caption>
</table>
</div>
</p>
<p><strong>MEC gives us a way of balancing both how likely we think each
theory is with how much each theory cares about our actions.</strong> We
can see that utilitarianism and deontology’s relative contributions to
the total moral value cancel out, and we are left with an overall “+10”
in favor of lying to save a life. This calculation tells us that—–when
accounting for how likely Alex thinks each theory is to be true and how
strong the theories’ preferences over his actions are—–Alex’s best guess
is that this action is morally good. (Although, since these numbers are
rough and the final margin is quite thin, we would be wary of being
overconfident in this conclusion: the numbers do not necessarily
represent anything true or precise.) MEC has distinct advantages. For
instance, unlike MFT, we ensure that we avoid actions that we think are
probably fine but might be terrible, since large negative
choice-worthiness from some theories will outweigh small positives from
others. This is a sensible route to take in the face of moral
uncertainty.</p>
<p><strong>However, MEC faces challenges when comparing
theories.</strong> While some cases are neatly solved by MEC, its
philosophical foundations are questionable. Our assignment of
choice-worthiness can be arbitrary: virtue ethics, for instance, advises
acting virtuously without clear guidelines. MEC is unable to deal with
‘ordinal’ theories that only rank actions by their moral value rather
than explicitly judging how morally right or wrong they are, making it
difficult to determine choice-worthiness. Absolutist theories, like
extreme Kantian ethics, deem certain actions absolutely wrong. We had
initially assigned -1000 for the value of lying to save a life for a
deontologist; it is unclear what we could put that would capture such an
absolutist view. If we considered this to be infinitely bad, which seems
like an accurate representation of the view, then it would overwhelm any
other non-absolutist theory. Even if we think it is simply very large,
these firm stances can be more forceful than other ethical viewpoints,
despite ascribing a low probability to the theory’s correctness. This is
because even a small percentage of a large value is still meaningfully
large; consider that 0.01% of 1,000,000 is still 100 — a figure that may
outweigh other theories we deem more probable.</p>
<p><strong>Following a moral parliament approach, Alex could consider
the proposal to lie more thoroughly and make a considered
decision.</strong> In the moral parliament, imagined delegates
representing different moral theories negotiate to find the best
solution. In Alex’s moral parliament, there would be 60 utilitarian
delegates, 30 deontological delegates, and 10 contractarian
delegates–—numbers proportional to his credence in each theory. These
delegates would negotiate and then come to a final decision by voting.
Drawing inspiration from political systems, the moral parliament allows
an agent to find flexible recommendations that enable compromises among
plausible theories.<p>
Depending on the voting rule, moral parliament can lead to different
outcomes. In a conventional setting with majority rule, the utilitarians
in Alex’s moral parliament would always be able to push through their
decisions. To avoid such outcomes, philosophers recommend using
<em>proportional chances voting</em>. Here, an action is taken with a
probability equal to its vote-share: in this case, if no one changed
their mind, then the outcome of the parliament would recommend with 70%
probability that Alex lie and with 30% probability that he tells the
truth, since only the 30 deontological delegates would vote against this
proposal. This encourages negotiation even if there is already a
majority, which naturally leads to more cooperative outcomes. This is a
more intuitive approach than, for instance, assigning choice-worthiness
values and multiplying things out, even if it does take more
effort.<p>
<strong>However, it is difficult to see what a moral parliament might
recommend.</strong> We can envision a variety of different possible
outcomes in Alex’s case. We might have the simple outcome described
above, where no one changes their mind. Or, we might think that the
deontologists can convince the contractualists that lying is bad in this
case because lying would not be tolerated behind the veil of ignorance,
reducing the chance of lying to 60%. Or, the parliament might even
propose something entirely new, such as a compromise in which Alex does
not explicitly lie but simply omits the truth. All of these are
reasonable—it is difficult to choose between them.</p>
<p><strong>The outcome of the moral parliament is not determined
externally.</strong> Instead, it is a matter of our imagination, subject
to our biases. Often, individuals resist imagining things that
contradict our preconceived notions. This means that moral parliament
may not be very helpful for individuals thinking about what is right to
do in practice. With enough resources, however, we might be able to
simulate model parliaments to recommend such decisions for us, such as
by hiring diplomats to represent moral positions and having them
bargain—or by assigning moral views to multiple AI systems and having
them come to a collective decision.</p>
<h3 id="conclusions-about-moral-uncertainty">Conclusions about Moral
Uncertainty</h3>

<h2 id="sec:implementing-moral-parliament">6.9.2 Implementing a Moral Parliament in AI Systems</h2>
<strong>AIs might use simulate moral parliaments to account for moral uncertainty.</strong>
If we want AIs to be guided by moral theories while accounting for moral uncertainty, we might use AI systems to simulate a moral parliament. 
We could imagine using advanced AI systems to emulate these representatives by training AIs to act in accordance with a specific moral theory. 
This would enable us to run moral parliaments artificially, permitting real-time decision-making. Just like in real parliaments, we would have delegates---AIs 
instructed to represent certain moral theories---get together, discuss what to do, negotiate favorable outcomes, and then vote on what to recommend. We could 
use the output of this moral parliament to instruct a separate AI in the real world to take certain actions over others.<p>
This is speculative and might still face problems; for instance, AIs
might have insufficient understanding of our moral theories. However,
these problems could become more tractable with advanced AI systems.
Assuming we have this ability, using a moral parliament might be an
attractive solution to getting AIs to act in accordance with human
values in real time.</p>
<strong>Moral parliaments could be useful for representing stakeholders, not just theories.</strong>
While we have explored the traditional moral parliament method of
representing moral theories, we can generalize beyond this. Instead of
representing theories, it might be more appropriate to represent
stakeholders; for instance, in a decision about public transport, we
could emulate representatives for local residents, commuters, and
environmental groups, all of whom have an interest in the outcome. Using
a generalized moral parliament for decision-making in AI is an approach
that ensures all relevant perspectives are taken into account. In
contrast to traditional methods that focus on representing different
moral theories, <em>stakeholder representation</em> prioritizes the
views of those directly affected by the AI’s decisions. This could
enhance the AI’s understanding of the intricate human social dynamics
involved in any given situation.</p>
<h2 id="advantages-of-a-moral-parliament">6.9.3 Advantages of a Moral
Parliament</h2>
<p>Using moral parliaments presents a wide array of benefits relative to
just giving AIs certain sets of values directly. In this subsection, we
will explore how they are customizable, transparent, robust to bugs and
errors, adaptable to changing human values, and pro-negotiation.</p>
<strong>Customizable
moral parliaments are diverse and scalable.</strong>
The generalized moral parliament can accommodate a wide variety of
stakeholders, ranging from individual users to large corporations, and
from local communities to global societies. By emulating a large set of
stakeholders, we can ensure that a diverse set of views are represented.
This allows AIs to effectively respond to a wide range of scenarios and
contexts, providing a robust framework for ensuring AIs decisions
reflect the values, interests, and expectations of all relevant
stakeholders. Additionally, moral parliaments are scalable: if we are
concerned about a lack of representation, we can simply emulate more
stakeholders. By grounding AI decision-making in human perspectives and
experiences, we can create AI systems that are not only more ethical and
fair but also more effective and beneficial for society as a whole.</p>
<strong>Transparency
is another key benefit of a moral parliament.</strong>
As it stands, automated decision making is opaque: we rarely
understand why AIs make the decisions they do. However, since the moral
parliament gives us a clear mechanism of representing and weighting
different perspectives, it allows stakeholders to understand the basis
of an AI’s decision-making. We could, for instance, enforce that AIs
keep records of simulated negotiations in human languages and then view
the transcripts. Using moral parliaments provides insights into how
different moral considerations have been weighed against each other,
making the decision-making process of an AI more transparent,
explainable, and accountable.</p>
<strong>Moral parliaments may result in AI systems that are less fragile and less prone to over-confidence.</strong>
If we are sure that utilitarianism is the correct moral view, we
might be tempted to create AIs that maximize wellbeing-—this seems clean
and elegant. However, having a diverse moral parliament would make AIs
less likely to misbehave. By having multiple parliament members, we
would achieve <em>redundancy</em>. This is a common principle in
engineering: to always include extra components that are not strictly
necessary to functioning, in case of failure in other components (and is
explored further in the chapter). We would do this to avoid failure
modes where we were overconfident that we knew the correct moral theory,
such as lying and stealing for the greater good, or just to avoid poor
implementation from AIs optimizing for one moral theory. For example, a
powerful AI told that utilitarianism is correct might implement
utilitarianism in a particular way that is likely to lead to bad
outcomes.</p>
Imagine an AI that has to evaluate millions of possibilities for every
decision it makes. Even with a small error rate, the cumulative effect
could lead the AI to choose risky or unconventional actions. This is
because, when evaluating so many options, actions with high variance in
moral value estimation may occasionally appear to have significant
positive value. The AI could be more inclined to select these high-risk
actions based on the mistaken belief that they would yield substantial
benefits. For instance, an AI following some form of utilitarianism
might use many resources to create happy digital minds—at the expense of
humanity-—even if that is not what we humans think is morally
good.</p>
This is similar to the Winner’s Curse in auction theory: those that win
auctions of goods with uncertain value often find that they won because
they overestimated the value of the good relative to everyone else; for
instance, when bidding on a bag of coins at a fair, people who
overestimate how many coins there are will be more likely to win.
Similarly, the AI might opt for actions that, in hindsight, were not
truly beneficial. A moral parliament can make this less likely, because
actions that would be judged morally extreme by most humans also
wouldn’t be selected by a diverse moral parliament.</p>
The process of considering a range of theories inherently embeds
redundancy and cross-checking into the system, reducing the probability
of catastrophic outcomes arising from a single point of failure. It also
helps ensure that AI systems are robust and resilient, capable of
handling a broad array of ethical dilemmas.</p>
<strong>Moral
parliaments encourage compromise and negotiation.</strong>
In real-life parliaments, representatives who hold different opinions
on various issues often engage in bargaining, compromise, and
cooperation to reach agreeable outcomes and find common ground. We want
our AIs to achieve similar outcomes, such as ones that are moderate
instead of extreme. Ideally, we want AIs to select outcomes that many
moral theories and stakeholders all like, rather than being forced to
trade off between them.</p>
In particular, we might want to design our moral parliaments in specific
ways to encourage this. One such feature is proportional chances voting,
in which each option then gets a chance of winning that’s proportional
to the number of votes it gets—if a parliament is 60/40 split on a
proposal, then the AI would do what’s recommended 60% of the time rather
than just going with the majority. This setup motivates the
representatives to come together on options that are compromises rather
than sticking to their own viewpoints rigidly. They want to do this to
prevent any option they see as extremely bad from having any chance of
winning. This ensures a robust high-level principle guiding AI behavior,
reducing the risk of extreme outcomes, and fostering a more balanced,
nuanced approach to ethical decision-making.</p>
<strong>Using
a moral parliament reduces the risk of overlooking or locking in certain
values.</strong>
The moral parliament represents an approach to ethical
decision-making in AI that is distinctively cosmopolitan, in that it
encompasses a broad range of moral theories or stakeholders. It ensures
that many ethical viewpoints are considered. This wider view is helpful
in dealing with moral problems and tough decisions AI systems may run
into, by making sure that all important considerations are thought over
in a balanced way. AIs using moral parliaments are less likely to ignore
values that matter to different groups in society.</p>
Further, the moral parliament allows the representation of human values
to grow and change over time. We know that moral views change over time,
so we should be humble about how much we know about morality: there
might be important things we don’t yet understand that could help us get
closer to the truth about what is right and wrong. By regularly using
moral parliaments, AI systems can keep up with current human values,
rather than sticking to the old values that were defined when the AI was
created. This keeps AI up-to-date and flexible, and prevents it from
acting based on outdated or irrelevant values that are locked into the
system.</p>
<strong>Challenges.</strong>
Deciding which ethical theories to include in the moral parliament
could be a challenging task. There are numerous ethical frameworks, and
selecting a representative set may be subjective and politically
charged. The decision procedure used to assign appropriate weights to
different ethical theories and aggregate their recommendations in ways
that reflect their importance could also be contentious. Different
stakeholders may have varying opinions on how to prioritize these
theories. Moreover, ethical theories can be subject to interpretation
and may have nuanced implications. Advanced AI systems would need to be
able to accurately understand and apply these theories in order to use a
moral parliament.</p>
<h3 id="conclusions-about-moral-parliament">Conclusions About Moral
Uncertainty</h3>
<strong>Summary.</strong>
<p><strong>Taking moral uncertainty into account is difficult but
important.</strong> As AI systems become increasingly embedded across
various part of society and take more consequential decisions, it will
become more important to ensure that they can handle moral uncertainty.
The same AI systems may be used by a wide variety of people with
different moral theories, who would demand that AI acts as far as
possible in ways that do not violate their moral views. Incorporating
moral uncertainty into AI decision-making could reduce the probability
of taking actions that are seriously wrong under some moral
theories.<p>
In this section, we explored ways of moving beyond intuitive judgments
or a reasonable pluralism of theories, examining three solutions to
moral uncertainty: my favorite theory, maximize expected
choice-worthiness, and moral parliament. Each approach has its strengths
and limitations, with the choice depending on the situation, level of
uncertainty, and personal preferences.
<p>We also discussed how we might use AIs to operationalize moral parliaments, whether in the original form of representing moral theories or generalized to representing 
stakeholders for any given issue. We highlighted the advantages of using a moral parliament, such as reducing the risk of overlooking or locking in certain values, allowing
 for the representation of changing human values over time, and increasing transparency and accountability in AI decision-making. We also noted that moral parliaments encourage
 compromise and negotiation, leading to more balanced and nuanced ethical decisions. 
<p>
It is important to recognize that there might not be a one-size-fits-all
solution to ethical dilemmas. As AI technology progresses, we should
establish methods for addressing moral uncertainty, such as including
diverse moral perspectives and quantifying uncertainty.</p>

<h1 id="conclusion">6.10 Conclusion</h1>
<strong>Overview.</strong>
In this chapter, we have explored various ways in which we can embed
ethics into AI systems, ensuring that they are safe and beneficial. As it
stands, it is far from guaranteed that the development of AIs will lead
to socially beneficial outcomes. By default, AIs are likely to be
developed according to businesses’ economic incentives and are likely to
follow parts of the law. This is insufficient. We almost certainly need
stronger protections in place to ensure that AIs behave ethically.
Consequently, we discussed how we can ensure AIs prioritize aspects of
our wellbeing by making us happy and help us flourish. Supposing AIs can
figure out how to promote individual wellbeing, we explored social
welfare functions as a way to guide their actions in order to help
improve wellbeing across society. Lastly, we considered using moral
parliaments to emulate democratic decision-making processes within AI
systems, ensuring ethical behavior by representing human interests
directly.</p>
<strong>We
should strive to make our views on machine ethics less
contradictory.</strong>
We have considered several possible perspectives on how to ensure AIs
act ethically. As we have seen, there are deep tensions between many
plausible views, such as “AIs should do what you choose” versus “AIs
should do what you want” versus “AIs should do what makes you happy” and
so on. It is quite difficult to resolve these tensions and choose which
version of machine ethics best represents human values; however, before
we deploy powerful AI systems, we must do so anyway.</p>
<strong>As a baseline, we
want AIs to follow the law.</strong>
At the very least, we should require that AIs follow the law. This is
imperfect: as we have seen, the law is insufficiently comprehensive to
ensure that AI systems are safe and beneficial. Laws have loopholes, are
occasionally unethical and unrepresentative of the population, and are
often silent on doing good in ways AIs should be required to do.
However, if we can get AIs to follow the law, then we are at least
guaranteed that they refrain from the illegal acts—such as murder and
theft—that human societies have identified and outlawed. In addition, we
might want to support regulation that ensures that AI decision-making
must be fair—once we have a better understanding of what fairness
requires.</p>
<strong>AIs
could increase human wellbeing in accordance with a social welfare
function.</strong>
Which conception of human wellbeing best matches reality and how we
should distribute it are difficult questions that we have certainly not
resolved within this chapter. However, if we had to guess, we might want
AIs to optimize continuous prioritarian social welfare functions where
individual wellbeing should be based on happiness or objective goods,
ensuring that wellbeing is fairly distributed throughout a society in
which everyone is happy to live in. We might use AIs to estimate
general-purpose wellbeing functions and directly increase what we
observe makes people better off. While this is speculative, the rapid
development of AIs forces us to speculate.</p>
<strong>AIs
might use moral parliaments for redundancy and adaptability.</strong>
Especially if we are unsure about what comprises human wellbeing and
how we should best distribute it, we might want to embed redundancy and
adaptability into our AIs so that they can act ethically even if we make
mistakes or change our minds. Given the potential benefits and the
ability of moral parliaments to address key concerns in AI
decision-making, we should be optimistic about their future use. By
incorporating diverse perspectives of individual moral theories or
stakeholders in real-time, moral parliaments can help ensure that AI
systems act in accordance with human values and avoid extreme or biased
behavior, even if human values change over time.</p>
<strong>Further work is needed on how to embed ethics into AI systems.</strong>
As we move forward, it is crucial that we continue to engage in
rigorous research, open dialogue, and interdisciplinary collaboration to
address the ethical concerns associated with AI. By doing so, we can
strive towards creating AI systems that not only avoid the worst harms
to society but actively work towards enhancing social wellbeing. In the 
next part of this textbook, we will move to the problem of how to ensure positive 
outcomes in a world with multiple AI agents and many, often competing, human stakeholders.</p>


<br>
<br>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-macaskill2020moral" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] W.
MacAskill, K. Bykvist, and T. Ord, <em>Moral uncertainty</em>. 2020.
doi: <a
href="https://doi.org/10.1093/oso/9780198722274.001.0001">10.1093/oso/9780198722274.001.0001</a>.</div>
</div>
<div id="ref-lloyd2022property" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] H.
R. Lloyd, <span>“The property rights approach to moral
uncertainty.”</span> [Online]. Available: <a
href="https://www.happierlivesinstitute.org/report/property-rights/">https://www.happierlivesinstitute.org/report/property-rights/</a></div>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-newberry2021parliamentary" class="csl-entry"
role="listitem">
<div class="csl-left-margin">[1] T.
Newberry and T. Ord, <span>“The parliamentary approach to moral
uncertainty,”</span> Future of Humanity Institute, University of Oxford,
2021. Available: <a
href="https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf ">https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf
</a></div>
</div>
</div>
