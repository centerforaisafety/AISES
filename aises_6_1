<!-- Machine Ethics -->

<h1 id="introduction">6.1 Introduction</h1>
<p><strong>How should we direct AIs to promote human values?</strong> As we continue to develop powerful AI systems, it is crucial to ensure they
 are safe and beneficial for humanity. In this chapter, we discuss the challenges of specifying appropriate values for AI systems to pursue. 
Some of these questions are already relevant for AI systems that exist today in the healthcare or automotive sectors, where artificial systems 
may be making decisions in situations that can harm or benefit humans. They will become even more critical for future systems that may be 
integrated more broadly across economies, governments, and militaries, making high-stakes decisions with many important moral considerations. As discussed in section 3.4, 
these questions are core components of the overall challenge of "AI alignment".
</p>
<p><strong>Many
people have incoherent views on embedding values into AIs.</strong>
People often talk about what AIs should do to promote human values.
They may agree with many of the following:</p>
<ol>
<li><p>AIs should do what you tell them to do.</p></li>
<li><p>AIs should promote what you choose to do.</p></li>
<li><p>AIs should do what’s fair.</p></li>
<li><p>AIs should do what a democratic process tells them to
do.</p></li>
<li><p>AIs should figure out what is moral, then do that.</p></li>
<li><p>AIs should do what is objectively good for you.</p></li>
<li><p>AIs should do what would make people happy.</p></li>
</ol>

<p>All of these seem like reasonable answers. At least at first glance,
these all seem like excellent goals for machine ethics. However, not all
of these are compatible, because they make <em>different normative
assumptions</em> and <em>require different technical
implementations</em>. Other suggestions such as “AIs should follow human
intentions” are highly vague. Put straightforwardly, while these sound
attractive and similar, they are not the same thing.<p>
This should challenge the notion that machine ethics is a
straightforward problem with a simple solution, and that the real
challenges lie only elsewhere, such as in making AI systems more capable. In
fact, those who believe there are easy ways to ensure that AIs act
ethically may find themselves grappling with inconsistencies and
confusion when confronted with instances in which their preferred
methods appear to yield harmful behavior.</p>
<p>
In this chapter, we will explore these issues,
attempting to understand which answers, if any, take us closer to
creating safe and beneficial AIs. We start by considering some goals 
commonly proposed to ensure that AI systems are beneficial for society. 
Current and future AI systems will need to comply with existing laws and 
to avoid biases and unfairness towards certain groups in society. Beyond 
this, many want AI to make our economic systems more efficient and to boost 
economic growth. We consider the attractions of these goals, as well as some shortcomings:</p>
<ol>
<li><p><strong>Law</strong>: Why not have AIs follow the law? We examine
whether we can design AIs that follow existing legal frameworks,
considering that law is a legitimate aggregation of human values that is
time-tested and comprehensive while being both specific and adaptable.
We will lay out the challenges of the occasional silence, immorality, or
unrepresentativeness of established law.</p></li>
<li><p><strong>Bias and Fairness</strong>: Should we make AIs be fair? We explore
fairness in AI systems and the challenges associated with ensuring
outcomes created by AIs are fair. We will discuss different definitions
of fairness and see how they are incompatible, as well as consider
approaches to mitigating biases.</p></li>
<li><p><strong>Economic Engine</strong>: Should we let the economy
decide what AIs will be like? We consider how economic forces are shaping AI development, 
and why we might be concerned about letting economic incentives alone determine how AI is developed
and which objectives AI systems pursue.</p></li>
</ol>

<p><strong>Machine ethics.</strong> While all of these proposals seem to capture important intuitions about what we value, we argue that they face significant limitations and are not sufficient on their own to ensure beneficial outcomes.
 We consider what it would mean to create AI systems that aim directly to improve human wellbeing. This provides a case study in thinking about 
how we can create AI systems that can respond appropriately to moral considerations, an emerging field known as machine ethics. While this is a highly ambitious goal, we believe 
it is likely to become increasingly relevant in coming years. As AI capabilities improve, they may be operating with an increasing level of autonomy and a broadening scope of 
potential actions they could take. In this context, approaches based only on compliance with the law or profit maximization are likely to prove increasingly inadequate in providing
 sufficient guardrails on AI systems' behavior. To specify in greater detail how they should react in a particular situation, AI systems will need to be able to identify and respond
 appropriately to the relevant values at stake, such as human wellbeing.</p>

<p><strong>Wellbeing.</strong> If we are to set wellbeing or other values as goals for AI systems, one fundamental question that faces us is how we should specify these values. In the second
 part of this chapter, we consider various interpretations of wellbeing and how attractive it would be to have AI systems pursue these, assuming that they became capable enough to do this. 
We start by introducing several competing theories of wellbeing, which might present different goals for ethical AI systems. We examine theories of wellbeing that focus on pleasure, objective
 goods, and preference satisfaction. We then explore preference satisfaction in more detail and consider what kind of preferences AI systems should satisfy. AI systems can be created to satisfy
 individual preferences, but which preferences they should focus on is an open question. We focus on the challenges of deciding between revealed, stated, and idealized preferences. Next, we 
turn to consider whether AI systems should aim to make people happy and how we might use AIs to promote human happiness. Finally, we consider the challenge of having AIs maximize wellbeing
 not just for an individual, but across the whole of society. We look at how to aggregate total wellbeing across society, focusing on social welfare functions. We discuss what social welfare 
functions are and how to trade-off between equity and efficiency in a principled way.</p>

<p><strong>Moral uncertainty.</strong> There are many cases where we may feel uncertain about what the right response is to a particular situation due to conflicting moral considerations. 
We consider how AI systems might respond to such situations. In the case of AI systems, we may deliberately want to introduce uncertainty into their reasoning to avoid over-confident 
decisions that could be disastrous from some moral perspectives. One option to address moral uncertainty is using a moral parliament, where ethical decisions are 
made by simulating democratic processes.</p>
